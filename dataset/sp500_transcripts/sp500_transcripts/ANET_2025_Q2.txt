Thank you Regina. Good afternoon everyone and thank you for joining us. With me on today's call are Jayshree Ullal, Arista Networks Chairperson and Chief Executive Officer, and Chantelle Brydup, Arista's Chief Financial Officer. This afternoon, Arista Networks issued a press release announcing the results for its fiscal second quarter ending June 30th, 2025. If you want a copy of the release, you can access it online at our website. During the course of this conference call, Arista Networks Management will make forward-looking statements, including those relating to our financial outlook for the third quarter of the 2025 fiscal year, longer-term business model, and financial outlooks for 2025 and beyond. Our total addressable market and strategy for addressing these market opportunities, including AI, custom demand trends, tariffs and trade restrictions, supply chain constraints, component costs, manufacturing output, Inventory management and inflationary pressures in our business lead times product innovation, working capital optimization, and the benefits of acquisitions, which are subject to the risks and uncertainties that we discuss in detail in our documents filed with the SEC, specifically in our most recent Form 10Q and Form 10K, and which could cause actual results to differ materially from those anticipated by these statements. These forward-looking statements apply as of today and should not rely on them as representing our views in the future. We undertake no obligation to update these statements after this call. This analysis of our Q2 results and our guidance for Q3 2025 is based on non-GAAP and excludes all non-cast stock-based compensation impacts, certain acquisition-related charges, and other non-recurring items. A full reconciliation of our selected GAAP to non-GAAP results is provided in our earnings release. With that, I will turn the call over to Jayshree. Thank you, Rudy. And thank you everyone for joining us this afternoon for our second quarter 2025 earnings call. Arista is experiencing momentum in our business as demonstrated in our record Q2 2025 results. We achieved $2.2 billion this quarter, surpassing our plan by $100 million. Software and service renewals contributed approximately 16.3% of revenue. Our non-GAAP gross margins of 65.6% was influenced by efficient supply chain and inventory benefit with a non-material tariff impact in the quarter. International contributions for the quarter registered strongly at 21.8%, with the Americas at 78.2%. Reviewing our mid-year inflection point, our conviction with AI and CloudTitans and enterprise customers has only strengthened. We began the year with a pragmatic guide of 17% or 8.2 billion annual revenue, but as the year has progressed, we recognize the potential to build a truly transformational networking company addressing a massive total available market. This feels to us like a unique once in a lifetime opportunity. We therefore raised our 2025 annual growth to 25%, now targeting 8.75 billion in revenue, which is an incremental 550 million more due to our increased momentum that we are experiencing across AI, cloud, and enterprise sectors. It is important to appreciate that Arista's AI Center strategy is complementing our data center focus to drive some of this increase. AI centers consist of both scale-out front-end and scale-up-scale-out combination for back-end networks. Scale-up back-end networks consist of high bandwidth, low latency interconnects that tightly link multiple accelerators within a single rack as a unified compute system with workload parallelism. Today, this is predominantly constructed with NVLink as a compute-attached I.O., but we do expect a move to open standards such as Ethernet or UA-Link in the next few years. Scale-out backend network is dedicated spines interconnecting XPUs across racks, engineered for high bandwidth and minimal latency, thereby resulting in efficient parallel processing of massive training models. Here, InfiniBand is rapidly migrating to Ethernet based on the UltraEthernet Consortium specification released in June of 2025. Scale-Out Frontend connects the backend clusters to external clouds, compute resources, storage, wide area networks, and data center interconnect to handle data ingestion, orchestration for AI, and cloud traffic in a leaf-spine network topology. Arista's flagship EtherLink and EOS are key hallmarks of scale-out networking with a wide breadth and depth of network protocol support. Introduced in 2024, Arista's EtherLink portfolio is now 20 plus products with the most comprehensive and complete solution in the industry, especially for scale-out back-end and scale-out front-end networking. It highlights our accelerated networking approach, bringing a single point of network control and visibility differentiation and improved GPU utilization. Poor networks and bottlenecks lead to idle cycles on GPUs, wasting both capital GPU costs and operational expenses such as power and cooling. With a 30% to 50% processing time spent in exchanging data over networks and GPU, the economic impact of building an efficient GPU cluster with good networking improves utilization, and this is super paramount. Our stated goal of 750 million backend AI networking is well on track and gaining from nearly zero revenue three years ago in 2022 to production deployments this year in 2025. As a reminder to you all, the back end AI is all incremental revenue and incremental market share to Arista. As large language models continue to expand into distributed training and inference use cases, we expect to see the back end and the front end converge and coalesce more together. This will make it increasingly difficult to parse the back end and the front end precisely in the future. but we do expect an aggregate AI networking revenue to be ahead of the 1.5 billion in 2025 and growing in many years to come. We will elaborate more on this in Analyst Day in September, including our AI strategy and forecast. What is crystal clear to us and our customers is that Arista continues to be the premier and preferred AI networking platform of choice for all flavors of AI accelerators. While majority today is NVIDIA GPUs, we are entering early pilots connecting with alternate AI accelerators, including startup XPUs, the AMD MI series, and in AI and Titan customers who are building their own XPUs. As we continue to progress with our four top AI Titan customers, AI is also spreading its wings into the enterprise and Neo Cloud sectors, and we are winning approximately 25 to 30 customers to date. The rise in agentic AI ensures any-to-any conversations with bidirectional bandwidth utilization. Such AI agents are pushing the envelope of LAN and WAN traffic patterns in the enterprise. So speaking of WAN, we are very pleased to announce the purchase of SD-WAN leader VeloCloud to offer modern branches in the agentic AI era. VeloCloud's secure AI-optimized WAN portfolio offers seamless, application-aware solutions to connect customer branch sites, complementing Arista's leading spines in the data center and campus. In a classic lead-spine atomic identifier, we are enabling multipapping, encryption, in-band network telemetry, segmentation, application identification, and traffic engineering across distributed enterprise sites. We are so excited to fill this missing void in our distributed enterprise puzzle to bring that holistic branch solution. This also increases our foothold with managed service providers, MSPs, as an important route to market for our distributed campus and branch offerings. We also intend to work closely with best-of-breed security partners to enable SASE overlays. Please do note that Velo is not material in 2025, and we have some work to do to restore annual revenue back to pre-Broadcom levels. Last quarter, I shared the development and internal promotions of several tenured executives at Arista to bolster our leadership. They display that strong cultural synergy and a mission to ignite innovation and delight our customers. As we enter the next phase of Arista 2.0, growing from $5.8 billion in 2023 to a forecasted $10 billion revenue in 2026, we rely on this trifecta foundation of great customers, innovative products, and great next-gen leaders to achieve this. I am so thrilled to welcome Todd Nightingale as Arista's President and Chief Operating Officer. Todd brings that incredible passion for networking with his over two decades of technical leadership in Meraki, Cisco, and most recently, CEO of Fastly. In just a month, he is epitomizing the Arista way, and I'm really looking forward to his impactful contributions to boost Arista's overall campus and enterprise operations. Todd, welcome to your first ANET earnings call. How does it feel to be here? It's amazing. It's only been a month, but I can't tell you how impressed I am with the passion and focus of the team, the trust that Arista customers have in the technology, and the enormous opportunity we have ahead of us in data center, AI, and in the campus. I'll be primarily focused on our enterprise customer engagement, bringing new customers to Arista and operational excellence across the organization. Personally, I'm so incredibly excited to be back in networking, and I'm truly, truly honored to be here. Thank you so much, Jayshree. Thank you, Todd. It's going to be a fun journey here with us. You know, it's really an unprecedented time in networking where Arista is so uniquely positioned to enable the modern network transformation. And with that, my dear friend Chantelle, over to you, our CFO, for the financial specifics. Thank you, Jayshree. With that as the backdrop of our strong business outlook, let me now take us through the metrics that underscore our momentum. Total revenues in Q2 were $2.2 billion, up 30.4% year over year, above our guidance of $2.1 billion. This was supported by strong growth across all of our product sectors. International revenues for the quarter came in at $481 million, or 21.8% of total revenue, up from 20.3% in the prior quarter. This quarter-over-quarter increase was driven by a relatively stronger performance in our EMEA region. The overall gross margin in Q2 was 65.6%, above our guidance of 63%, up from 64.1% last quarter, and up from 65.4% in the prior year quarter. The quarter over quarter gross margin improvement was primarily driven by improved inventory management and related excess and obsolescence reserves. Operating expenses for the quarter were $370.6 million, or 16.8% of revenue, up from last quarter at $327.4 million. R&D spending came in at $243.3 million, or 11% of revenue, up from $209.4 million in the last quarter. This primarily reflected higher new product introduction costs in the period. Sales and marketing expense was $105.3 million, or 4.8% of revenue, compared to $94.3 million last quarter, inclusive of a continued focus on our partner programs. Our G&A cost came in at $22 million, or 1% of revenue, down from last quarter at $23.7 million. Our operating income for the quarter was $1.08 billion, crossing $1 billion for the first time in Arista's history, landing at 48.8% of revenue. Other income and expenses for the quarter was a favorable $88.6 million, and our effective tax rate was 20.7%. This resulted in net income for the quarter of $923.5 million, or 41.9% of revenue. Our diluted share number was 1.271 billion shares, resulting in a diluted earnings per share number for the quarter of 73 cents, up 37.7% from the prior year. Now onto the balance sheet. Cash, cash equivalents, and investments ended the quarter at $8.8 billion. In the quarter, we repurchased $196 million of our common stock at an average price of $80.70 per share. Of the $1.5 billion repurchase program approved in May 2025, $1.4 billion remains available for repurchase in future quarters. The actual timing and amount of future repurchases will be dependent on market and business conditions, stock price, and other factors. Now turning to operating cash performance for the second quarter. We generated approximately $1.2 billion in cash from operations in the period, the highest in Arista's history, reflecting a strong business model performance. DSOs came in at 67 days, up from 64 days in Q1, driven by billing linearity. Inventory turns were 1.4 times flat to last quarter. Inventory increased to $2.1 billion in the quarter, up from $2 billion in the prior period, reflecting an increase in our finished goods inventory, which is an outcome of our global tariff and supply chain management. Our purchase commitments and inventory at the end of the quarter totaled $5.7 billion, up from $5.5 billion at the end of Q1. We expect this number to stabilize as supplier lead times improve, but will continue to have some variability in future quarters as a reflection of demand for our new product introductions. Our total deferred revenue balance was $4.1 billion, up from 3.1 billion in Q1. The majority of the deferred revenue balance is services related and directly linked to the timing and term of service contracts, which can vary on a quarter-by-quarter basis. Our product deferred revenue increased approximately $687 million versus last quarter. We remain in a period of ramping our new products, winning new customers, and expanding new use cases, including AI. These trends have resulted in increased customer-specific acceptance clauses, and an increase in the volatility of our product deferred revenue balances. As mentioned in prior quarters, the deferred balance can move significantly on a quarterly basis, independent of underlying business drivers. Accounts payable days was 65 days up from 49 days in Q1, reflecting the timing of inventory receipts and payments. Capital expenditures for the quarter were $24 million. In October 2024, we began our initial construction work to build expanded facilities in Santa Clara And we expect to incur approximately $100 million in capex during fiscal year 2025 for this project. Now turning to guidance. Building on this strong Q2 first half performance, we expect continued momentum in the quarters ahead. Let's first start with our outlook for fiscal year 2025. As Jayshree mentioned, revenue growth is now estimated to be approximately 25% or $8.75 billion. This is fueled by demand across AI, cloud, and enterprise sectors and demonstrates that Arista's focus on pure play networking is meeting the innovation needs of the market. One item to note, of this revenue guide raised, we are now increasing our campus revenue target to be between $750 and $800 million, inclusive of the minimal amount expected from the VeloCloud acquisition in FY25. We are excited to welcome VeloCloud to our team, and as stated earlier, We are working through integrating and enhancing the business model to better serve our customers. For gross margin, a range is expected of approximately 63% to 64%, inclusive of possible known tariff scenarios and benefiting from improved inventory management. For operating margin, the outlook is approximately 48%, a testament to the ability of Arista to scale efficiently and effectively. Given the strength of our business and visibility into customer demand, here is our guidance for the Q3 quarter. Revenue of approximately $2.25 billion, continuing to serve our customers and win new logos across AI, data, WAN, and campus centers. Gross margin of approximately 64%, inclusive of possible non-tariff scenarios. Operating margin of approximately 47%. and an effective tax rate expected to be approximately 21.5% with approximately 1.275 billion diluted shares. In closing, this is a great time to be an innovative networking leader. We are halfway through the year with solid momentum and are clear on our execution priorities. This makes us confident and excited in our ability to finish the year strongly. In closing, I would also like to wish Todd a very warm welcome to the industry team. I will now turn the call back to Rudy for Q&A. Thank you, Chantel. We will now move to the Q&A portion of the Arista earnings call. To allow for greater participation, I'd like to request that everyone please limit themselves to a single question. Thank you for your understanding. Regina, please take it away. We will now begin the Q&A portion of the Arista earnings call. To ask a question during this time, simply press star and the number 1 on your telephone keypad. If you'd like to withdraw your question, press star 1 again. please pick up your handset before asking questions to ensure optimal sound quality. Our first question will come from the line of George Nodder with Wolf Research. Please go ahead. Hi, guys. Thanks very much. Appreciate it. I guess I wanted to, the results are terrific, certainly, but I wanted to ask about the competitive environment. I know many investors in recent weeks and months have been, you know, looking at some of the growth at Celestica and certainly NVIDIA's networking business and kind of projecting some of that strength on, you know, as being negative for Arista. I guess I was just curious about, you know, your perspective on that, you know, how you see the competitive environment, how you see your differentiation, and anything you can say there would be great. Thanks. Sure, George, and welcome to both. Thank you for the wishes. Look, we've always lived in a very competitive industry, whether it was, it is, or I shouldn't say was, Cisco or specific networking vendors, and We acknowledged NVIDIA's participation both within Cineband and bundling with the GPUs. We've always acknowledged the coexistence with Whitebox. So from our perspective, the competitive landscape has not changed. It's more of the same. But I recognize that the chatter was louder. And we understand that given the volatility of some of our customers, Some years and some quarters are better. So I think some of the chatter was louder because our meta share wasn't growing the same way as it did year over year the prior years. But from our perspective, our innovation and differentiation has never been stronger at a platform performance level, at a feature level. And I want to add a third one, which is at a customer intimacy level. You know, they are so appreciative of the support, the quality, and the way we approach how to solve their problems. So no change in our environment and innovation. There's plenty of chatter outside. I appreciate that. I understand that. And I hope we've proved the naysayers wrong. Debra, thank you. Thank you, George. Our next question comes from the line of Meeta Marshall with Morgan Stanley. Please go ahead. Great. Thanks. appreciate the question I guess just on some of the strength that you're seeing in terms of cloud I know it's getting increasingly hard to kind of differentiate front end and back end but you know do you attribute some of the upside that you're seeing this year towards starting to see front end upgrades maybe quicker than expected or is this just kind of back end demand being stronger than expected thanks Thanks, Mita. If you recall two or three years ago, maybe it's hard to remember all of that, I was actually very worried that the cloud spending had a little bit frozen and all of the excitement and enthusiasm was going towards GPU and how big is your GPU cost or that kind of thing. We now see it coming back and the pendulum swinging into a more balanced deployment of both cloud and AI. And I think as a result of all these AI deployments, as I've often said, the traffic patterns of cloud and AI are very different. The diversity of the flows, the distribution of the flows, the fidelity of the flows, the duration, the size and intensity. So all of that AI traffic and deployments we have done and others have done is now putting pressure on the front end cloud as well. So that's why it's going to get, we wanted to measure ourselves as purely on the native GPU connections, but going forward, we see a much more distributed topology of cloud and AI sort of combining together. And it's not like HPC clusters where they'll build one and tear it down. When they build an AI cluster, it's very expensive. It's like diamonds, and they want to take advantage of that and bring it forward to other cloud resources as well. So to the point, the question you were asking, our increased $550 million had a little bit of velo, not material, as Chantal reminds me, but a lot of cloud and AI as well as enterprise campus. Great. Thank you. Our next question comes from the line of Ryan Coons with Needham & Company. Please go ahead. Great, thanks. I want to ask maybe a question for Todd, Jayshree, just about the fit for Velo with your traditional go-to-market motion, which has been heavily direct, and do you expect Velo to really beef up your channel efforts in working with these MSPs? Can you expand that a little bit? Thank you. Yeah, I think it's incredibly complimentary on two fronts. One is It fills an enormous hole in the enterprise campus portfolio for the distributed branch. And being able to bring Velo technology through our traditional Arista channel, it gives us an opportunity to, you know, cross-sell ST-WAN into so many existing campus accounts with the existing Arista go-to-market, which is amazing. But Velo has a really strong MSP motion. We're pushing really hard right now and really embracing that, not just to continue the Velo success, but to now bring all of Arista's portfolio through that same channel, through those same partners, and really embrace that MSP motion and use the Velo intellectual property in their business operation in order to learn from that and bring that MSP motion to all of Arista's portfolio. Well said, Todd. You know, sometimes with the engine and sometimes with the caboose. In the case of the MSP, you know, we're definitely going to leverage the strength of Velo. That makes sense, Cheshiree. Thanks so much. Our next question comes from the line of Antoine Gaben with New Street Research. Please go ahead. Hi. Thank you very much for the question. Can you please remind us what's required for scale-up topologies and how that differs from scale-ups. Are traffic patterns more predictable, easier to manage, and how do you see the competitive dynamics evolving? Does this create more differentiation for a restyle or more room for white box compared to scale-ups? Yeah. So, first of all, I would say that scale-up is a new and unique requirement. And it particularly is going to come in as people start building more and more AI racks, right? So when you're building an AI rack and you want to boost the radix and performance of an individual rack or cluster and your XPU radix gets bigger and bigger, you often need a very simple interconnect, right? This interconnect in the past has been PCIe Express. CXL, and now you're seeing a lot of NVIDIA NVLink where you can really collapse your system board and an XPU socket into an I.O. It's almost not a network. It's an I.O. It's a back-end to a back-end, if I can call it that. And so scale-up networks will be an incremental new market as Arista pursues it. Today, majority of that market lies inside a compute network structure and isn't something Arista is participating in. But we are very encouraged by the standards for scale up Ethernet that Broadcom has initiated and we're big fans of. We think that Ethernet as a transport protocol is going to favor Arista and Broadcom very much. And any little bit of incremental share we get there will be better than the zero we have right now. We also think UA-Link is another spec that's coming out, and that may run as an overlay on top of an Ethernet underlay. There needs to be some firm standards there, because today, scale-up is frankly all proprietary in V-Link, and we're encouraged by, just like we worked hard to found the UltraEthernet Consortium as a member for some of the back-end Ethernet and the migration from Infidiband to Ethernet is literally happening in three to five years, we expect the same phenomena on scale-up. Thank you, Jashree. Thank you. Our next question comes from the line of Amit Daryanani with Evercore. Please go ahead. Thanks a lot, and congrats on a nice set of numbers here. You know, Jesse, as I think about the 25% guide that you folks are talking about for the full year, which is really impressive, can you just talk about what are you seeing specifically that's enabling you to raise your guide from 17% to 25%? Just, you know, what markets or what vectors are you seeing that make you feel better about it? And then do you see the potential for this higher growth to be more durable as, you know, Arista realizes once-in-a-lifetime opportunity, as you talked about in your call. Thank you. Okay. Well, thank you for the wishes, Ahmed. As you know, it's not easy to execute on large numbers, so durable growth gets harder and harder as the numbers get bigger and bigger, but... We've always believed in a CAGR of mid-teens. We've always believed in double digits, so we hope we will continue to grow for many years to come in those kind of numbers. Coming back to your question, I think when we guided the year pragmatically back in February, what Chantelle and I saw was a lot of activity but not a lot of confirmation. Sitting here in August now, that activity has translated. in all three sectors into a lot of confirmations. Enterprise campus, I couldn't be more bullish. We had a record quarter in terms of demand. Obviously we have to shift, but it's the strongest we felt. And as Todd might appreciate, since he created a lot of Meraki, You know, as you look at the campus, this is going to be very strategic, very large and very important because it's a large TAM of 25 to 30 billion. So getting new logos, getting, you know, our value and our differentiation, especially in the post-pandemic era, understood in terms of wired, wireless, IoT, segmentation security, zero trust, zero touch provisioning was critical. We saw that shift happen in the first half of this year. On AI, I don't need to tell you that despite losing one of our key anchor customers, the fifth customer was a sovereign AI customer that's pretty much out of these numbers. We were still able to, we believe, achieve $750 million in back-end targets revenue and exceed $1.5 billion for the year. Exact numbers we'll know when we finally ship. We can't give you those specifics now. But despite losing one customer, we're having a lot of activity in the four big ones. And, you know, it's pleasantly a surprise to us to see the advent of enterprise and even some new clouds. Numbers are small. It's not as big as the large titans, but it's all adding up. And then the third thing, as I was telling Mita, was the cloud itself. When you start putting that kind of pressure on performance and bandwidth and capacity on the back end of the network, eventually you've got to go refresh the front-end cloud. So we're seeing many more migrations from 100 to 400 gig and even 800, and that's helping. So all three are contributing to this new growth we are projecting for the year. Perfect. Thanks. Our next question comes from the line of Michael Ng with Goldman Sachs. Please go ahead. Hey, good afternoon. I just have one and one follow-up. I guess for Chantel, I'm just wondering if you could just comment a little bit more on the deferred revenue or the billings growth. You know, what was the primary driver there? And I was wondering if that was a contributor to the magnitude of the guidance increase that we saw. And then second, you know, Jayshree, you mentioned the path towards, you know, $10 billion in 2026, you know, quite a ways out, but maybe you can just speak to some of the things that you're seeing. I know you've talked a lot through it, but, you know, the confidence in that number and, you know, things that could break in the right way that could result in that number even being better. Thank you. Yeah, I think for the first one, thank you for your question, is in the sense of from the deferred balance, you know, between product and services, this is indicative of new product, new use case, AI, as I mentioned in my remarks, and it's across those categories. And as far as this year, you know, the defer is going to be at this year, next year, because it's 12, 18, 24 months, some of the use cases that we have in there, to your point. So, you know, it's always helpful to have the deferred revenue balance. balance growing. It does move and does move with volatility, given some of the sizes of some of these new use cases and new product introduction. So more to come, but we don't guide it. So I would say that those are the factors that go into it for your question. And then Jayshree, did you want to mention? Yeah. Thank you, Michael. A couple of things. Even on 25, I think the parallels for me too, because I'm such a historian and I have so many years behind me, you know, if you look at the cloud, we had a lot of deferred. And, you know, on one hand, I can tell you guys, don't pay attention to it. It'll eventually come out and something will come in. But it is... a very high level of experimentation with new GPUs, traffic patterns, the number of GPUs, the location of the GPUs, the distribution of GPUs, the traffic patterns. There's a lot more work going on there. And customers are experimenting. Customers are seeing GPUs every 18 months. We have to adapt to that. We have to look at performance. We have to look at high availability. We have to look at automation visibility. So it's a non-tremial amount of complex work. And oh, by the way, often these are not brand new. It's part of a brownfield where we're trying to do all this. You know, the car is running at 100 miles an hour and we're trying to add AI to it kind of problem. So don't underestimate that this deferred that's been going on since last year will continue this year and perhaps next as well. And the length of deferred is taking longer. So on one hand, I'll say don't pay too much attention to it. Sometimes it'll come out. Sometimes it'll go in. On the other hand, this is definitely more because of AI than anything else. In terms of 2026, I think it's only fair we save Analyst Day for that. We did announce that. But look, I would be remiss if I didn't tell you I'm very proud of the team, and we're looking to achieve $10 billion in revenue in 2026, two years ahead of schedule. I promised you guys that back in the last Analyst Day in 2028. So there you go. That's the headline. That's very clear. Thank you, Jayshree. Thank you, Chantel. Thank you. Thank you. Our next question comes from the line of Simon Leopold with Raymond James. Please go ahead. Thank you very much. I know you don't like to give us specifics on customer contributions. I guess what I'm really trying to see if you could help us is understanding how that might be shifting given that it does seem like there should be some broadening with the neoclouds, the sovereigns and enterprise. But at the same time, some of your biggest customers are growing their spending significantly. So any clues or hints or quantification you can offer to help us better appreciate what your concentration and largest customer mix looks like and is trending towards? Yeah, I'll try my best. But the minute we call them a titan, which we now have included some customers in the Titan that previously weren't, and we've moved customers out of the Titans into the specialty providers, you know they have a big spend. So you should not be surprised to see at least 10% concentration from our two favorite customers. And we will get greater contribution from our other customers, even if they're not 10%, because of the AI investments. So our AI Titans, if you will, and our Cloud Titans are going to make... a meaningful, indeed high contribution to the earth. That's one half of us. That's one half of the coin. The other half is, you know, why we're so excited to have Todd. Don't underestimate the power of all these customers coming together as an aggregate, adding to a very high number. So it's not your one Titan. As a collection, they're a Titan, but each one of them by themselves are a meaningful contributor. So what Todd's team, along with Chris Schmidt, Chris Bellmere, Ashwin are doing is just fantastic. So we're really going to have a balanced approach of two very meaningful businesses contributing together. But definitely AI is going to create that. Large investments, large CapEx that you guys have all seen from our customers is going to translate into some investments into us too. We're equally excited about the enterprise. Thank you. Our next question comes from the line of Tal Liani with Bank of America. Please go ahead. Hi, guys. I want to talk about the sustainability. Hi, hi, hi. I want to talk about the sustainability of growth. Tomahawk 6 was delayed. And the question is whether there is any correlation between the delays in Tomahawk 6 and your growth. Are customers buying more now than before? Maybe they waited for it. And then also another follow-up on the sustainability of growth is also sustainability of margins. At 49%, almost 49% operating margin, when do you start to upset your customers, your big customers, because they have an alternative to buy white boxes and it's cheaper? Do you have that much of a differentiation that justifies paying a lot more for a product versus white boxes, assuming that on white boxes, the manufacturer doesn't make 49% margin? OK, tell us a loaded question on sustainability. So have we had sustainability over the last 15 years? Have we had white box over the last 15 years? I mean, I'm asking these questions rhetorically. I'm not expecting you to answer them. But look, it's going to be competitive, and there's a set of throwaway white boxes that some set of ODM manufacturers will build where they don't need all the value. And particularly in the LEAF situation, we can see that. If you don't need features, you don't need value, then you probably won't pay the premium price. but i also want to add that 49 percent mark operating margin is not a function of just our value it's a function of our efficiency this company knows how to do more with less we don't just throw thousands of marketing people sales people or engineers on one problem we architected correctly and we've always been efficient and i challenge you to find somebody some other company that does it more efficiently so that's not a white box problem that's an efficiency and our customers appreciate that we don't have layers and hierarchies and big company corporate stuff and we do this efficiently so they're kind of two different things no doubt we will coexist with white box no doubt a set of customers will appreciate our support our quality our innovation and would be willing to pay the premium because as i've often said to you too as well tell you can trade capex for opex and vice versa You can buy a cheap box, and then you can support it yourself, and you're going to need hundreds of engineers to do that. That's one model. And the other is RISTA, where we'll put in the buffers, the congestion control, the value, the EOS, and hopefully you will need less support staff to do that. You did ask me about Tomahawk 6. I mean, Broadcom's been a fantastic partner. I don't think they're late on it. This is very complex silicon technology. Tomahawk 6 is in our labs. Stay tuned for new product next year. Got it. Thank you. Thank you. Our next question comes from the line of James Fish with Piper Sandler. Please go ahead. Hey, great quarter. J3, for you, we've talked a little bit in the past about blue box instead of white box, I guess. What are you seeing on sort of the blue box side versus full system with some of your main customers? And Todd, sorry, you can't escape me here. You know, the VeloCloud side of things, obviously that space has evolved where it's gone into the SASE mode. Jayshree, you even said like, hey, we're going to partner with our security partners for a full SASE. But do you need to think about that more directly just because it is becoming a world where customers are looking for a full SASE offering from one throat to choke, as they say. I guess, how are you thinking about a broader savvy offering as opposed to just having the SD-WAN part? Thanks, guys. Do you want to take the first one first while I figure out this first question? Or the second one, yeah. Yeah. We are looking very carefully at how we support customers from a fully integrated, sassy SD-WAN solution. It's a secure WAN that matters. And Delivering that solution with great assurance is something that certainly is top of mind for us, but I think we have a real opportunity to do that with partners, partnership. There's so many amazing cloud security vendors out there right now, and we have so many customers that work with the Velo solution along with those partners. That's, I think, the way we're going to be leaning in moving forward, but certainly we'll be talking more about that in Investor Day later this year. Yeah, just to add to what Todd said, James, We see the bifurcation of SD-WAN sort of, there's a fork in the road in two ways. One is where there's a security angle on it. And if it's just simple security, encryption, segmentation, a firewall, we can do that. But if it's really the cloud security like Zscaler or Apollo Alto do, we will absolutely work with best-of-breed partners and not pretend to be something we're not. So our branch infrastructure to support security is very much an Arista priority. Our branch infrastructure, as Todd said, to become a SASE, a secure WAN, is an overlay on top of that that we work with our partners. But we really see, like I said, that fork in the road where SD-WAN isn't just a SASE solution, it's also a branch solution. When you have all these campuses with large headquarters and then your home is a branch, your retail is a branch, your library is a branch, you need a mini-me solution of our campus. And this is where I think VeloCloud will really shine with Arista products, with our wired wireless and bringing all of that cloud vision and VeloCloud orchestrated together for a seamless provision is a big goal of us. All the way from the multi-domain cloud vision to the cognitive unified edge and experience down to the branch. So we're excited about that fork in the road in one-wheel partner and in one-wheel build more integration ourselves. Blue box, very much an important part of our strategy. Still in strategy form, we expect to see that evolve in the next few years. We haven't had to build that muscle yet because we're still in crazy AI mode, but we absolutely will complement and coexist with the white box to offer a rift of blue box. And what do I mean by that? It means a very battle-tested, highly well-designed hardware, the way Andy Beckleschein and his team know how to do, can be delivered, you know, as an upgrade or as a better hardened white box. And we fully plan to do that. And, in fact, do do that with bundled software today. Thank you, Chris. Thank you, James. Our next question comes from the line of Samik Chatterjee with JPMorgan. Please go ahead. Yep. Hi. Thanks for taking my question. Jayshree, strong set of results here and congratulations on the strong outlook as well. If I go back to your comments about the ability to meet the 750 million AI backend number, even as the fifth customer is absent now, is that largely stemming from bigger cluster size deployments from your existing tier one customers or is something else moving around in terms of timing of those deployments related to expectations? And just as a follow-up, I think the fourth customer you had earlier reference was much slower in terms of So can you just give us an update on that front? Thank you. That's a good question, Sami. Thank you for the wishes as well. So I think two of our customers have already approached or are going to fast, quickly approach 100,000 GPUs. But I don't think it's any more about just how big. We used to talk about a million GPUs and all that. Increasingly, what we're seeing is more and more distributed GPU clusters for training and inference And so two customers have reached that goal. The third one might reach that goal. The fourth one that I said we just begin with is probably too early to reach that 100,000. That's probably a goal for next year. So that's the composition. Two are strong, one is medium, and the other is still low. But to make that number or actually to exceed that number, you may have noticed that I pointed out that we now have in an aggregate, I think last time we said 15, and now we're saying 25 to 30 enterprise and neocloud customers. So they're not big individually, but together they add up to contribute as well for the loss of the fifth customer and the slowness of the fourth. So we believe with the increase in 550 million that AI will be a contributor to that and exactly how it will shape up will depend on what we ship out. But feeling really good. And I won't measure it anymore just on number of GPUs. I think there's a lot more to do with locality, distribution, radix. And, you know, also choice of multi-tenants, optimizations, collective libraries, level of resilience, et cetera. So we're seeing a lot more complexity run into this than straight number of TPUs. Thank you. Thank you. Our next question comes from the line of Aaron Rakers with Wells Fargo. Please go ahead. Yeah, thanks for taking the question. Also, congrats on the quarter. This probably builds on a few other earlier questions, but, you know, Jayshree, I'm curious as we think about the sovereign AI opportunity, whether or not that's factoring at all into kind of what you're seeing currently. I know you alluded to a fifth customer, which was a sovereign falling out, but I'm curious to how you think about that opportunity set, what you're seeing as far as customer engagement. And, you know, if we should kind of think about that as becoming a more material, you know, incremental driver as we look through 2026 and beyond. Thank you. Yeah. No, Aaron, that's a good point. We've once bitten, twice shy. So since our fifth customer was a sovereign AI and it didn't work out, we're certainly not factoring it into our numbers this year. But we haven't lost faith or hope that that could be an important segment for us in the next several years. I think there's going to be a lot of expanded build-outs. In fact, one of the neoclouds is a sovereign AI, which is a non-NVIDIA cluster that we're working with right now that may factor in in 2026. But having said that, it's still early days, and we're cautiously optimistic. Thank you. Our next question comes from the line of Atif Malik with Citi. Please go ahead. Hi. Thank you for taking my question. Jayshree, you talked about scale up Ethernet to be incremental to your TAM. Curious if you have any sense how big this TAM is in three years. Atif, I don't know yet. In terms of port density, in terms of units, you know, if I look at the ratio within Iraq versus outside in units, it's quite high, 8 to 1, 10 to 1. But in terms of dollars, I don't think it's nearly as much because the level of functionality required is much simpler. So how about we beg that question out for September when we'll know more? That's the deal. Thank you. Okay. Thank you. I owe you one answer. Our next question will come from the line of Carl Ackerman with BNP Paribas. Please go ahead. Yes, thank you. Jayshree, you noted you are seeing good activity with the top four hyperscalers. While you indicated that your backend revenue this year will be primarily driven by two of them, would you expect that all four cloud providers would adopt ERISA switches for backend deployments in 2026? And I guess, where are you seeing the most opportunities with these NeoCloud providers? Because that certainly could be a big opportunity as we see in time. Thanks. So, Carl, the short answer would be yes. We've got some work to do, but the answer is absolutely. All four of them, two of them already have large, and the other two will be deployed in the back end. It will also fuel the front end. And in terms of NeoClouds, almost always the NeoCloud is a combination of back and front. It's never one or just the other, but definitely the new clouds also have a back end component. Thank you, Hino. We have one last question. Thank you. Our final question will come from the line of David Vogt with UBS. Please go ahead. Great. Thanks, guys, for squeezing me in. Jayshree, I just wanted to maybe pick your brain a little bit. You mentioned scale up, but can we talk about the competitive or maybe the technical opportunities with scale out with Jericho 4 that was announced today or yesterday and how you're thinking about kind of what that means for your technology position with regards to sort of distributed AI going forward. I know scale up's an incremental opportunity, but maybe just kind of share your thoughts on where you stand scale out. This is a really good, thoughtful question because this is our bread and butter. Arista is the premier scale-out spine platform. The 7800 spine, our AI spine, is a really flagship franchise platform. It takes advantage of all of the virtual output queuing, the congestion control, the perflow queuing, the buffering, et cetera, in a way that nobody else in the industry is able to demonstrate. And oh, by the way, besides being a great AI spine, it's also a great routing platform for the WAN. so this product is sort of the anchor for a lot of things we do at scale out both on the back end and front end and has been our workhorse for some time and it's only getting much of what we've done so far is 400 gigs with jericho 4. congratulations work home we're looking forward to the 800 gig and then in the and then beyond for others as well so thank you for reminding us that you know we're continuing to push the envelope of innovation and we fully expect um you know, the series that we started with R1, R2, R3 to evolve to R4, all in the context of a very consistent software and platform architecture. Thank you. This concludes Arista Network's second quarter 2025 earnings call. We have posted a presentation that provides additional information on our results, which you can access in the investor section of our website. Thank you for joining us today and for your interest in Arista. Thank you for joining, ladies and gentlemen. This concludes today's call. You may now disconnect.